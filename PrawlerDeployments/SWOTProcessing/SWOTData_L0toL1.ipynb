{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb98f9c-9265-489e-9dea-f81effcbd162",
   "metadata": {},
   "source": [
    "# SWOT Data Preparation\n",
    "\n",
    "\n",
    "## Datasource\n",
    "- http://redwing.pmel.noaa.gov:8080/erddap\n",
    "\n",
    "\n",
    "## Steps:\n",
    "\n",
    "### Data Steps\n",
    "1) Ingest Prawler Data - convert units if necessary (this is true for conductivity)\n",
    "2) Ingest Met Data (not currently used)\n",
    "3) Ingest GPS Data\n",
    "\n",
    "### Variable Meta Data\n",
    "2) standard names , units \n",
    "3) serial numbers, models\n",
    "\n",
    "### Project Meta Data\n",
    "1) Project\n",
    "2) PI\n",
    "3) Contacts\n",
    "4) Serials\n",
    "\n",
    "### Reference Values\n",
    "1) Clock Date is 20000101T000000Z\n",
    "2) all values are float64 or char\n",
    "\n",
    "### Synthesized Data Hosting (L1)\n",
    "\n",
    "Data Format was suggested by JPL - its not quite CF but close... rehost on local system as a test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ad17a-0392-42d3-a871-0c443762b685",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "Wrap successful code in modularized class for convenience and commenting\n",
    "- FileName Structure: SWOTPOSTLAUNCH_L1_MOORING-S1_CTD-PROFILER_START20190905_END20190905_VER001.nc\n",
    "\n",
    "Next Steps:  \n",
    "1 - What frequncy of file generation / update do we want  (3-6hr)  \n",
    "2 - what time base will be within each file  (daily with rewrites is fine)  \n",
    "3 - how to upload to JPL server  (host on pmel ftp site)  \n",
    "4 - error checking and comparison against RUDICS / SUMMARY FILE? -> visual confirmation file  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acb0f3c-a537-467b-b50a-e068b54ceeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use yaml to setup the instrument parameters\n",
    "\n",
    "import yaml\n",
    "\n",
    "stream = open('SWOTprawler.yaml', 'r')    # specifies the SWOT prawler/gps netcdf variable attributes\n",
    "SWOTprawler = yaml.safe_load(stream)\n",
    "\n",
    "stream = open('SWOTmet.yaml', 'r')    # specifies the SWOT met netcdf variable attributes, seperated because it isn't defined by NASA and isn't used\n",
    "SWOTmet = yaml.safe_load(stream)\n",
    "\n",
    "stream = open('SWOTGlobalAttrs.yaml', 'r')    # specifies the SWOT global netcdf variable attributes\n",
    "SWOTGlobal = yaml.safe_load(stream)\n",
    "\n",
    "SWOTSerialID = {'N001':{'Name':'SWOT N001','DepLatN':47.6062,'DepLonE':-122.3321,'InstMake':'Sea-Bird Scientific','InstModel':'GPCTD','InstSerial':'0000'},\n",
    "               #  'N201':{'Name':'SWOT N201','DepLat':None,'DepLon':None},\n",
    "               # 'N202':{'Name':'SWOT N202','DepLat':None,'DepLon':None},\n",
    "               # 'N203':{'Name':'SWOT N203','DepLat':None,'DepLon':None},\n",
    "               # 'N204':{'Name':'SWOT N204','DepLat':None,'DepLon':None},\n",
    "               # 'N205':{'Name':'SWOT N205','DepLat':None,'DepLon':None},\n",
    "               # 'N206':{'Name':'SWOT N206','DepLat':None,'DepLon':None},\n",
    "               # 'N207':{'Name':'SWOT N207','DepLat':None,'DepLon':None},\n",
    "               } #deployment specific information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9deaac-250b-476d-9efb-34faaba521f9",
   "metadata": {},
   "source": [
    "## Connect to EDD ERDDAP and retrieve all SWOT related datasets\n",
    "\n",
    "There are erddaps for GPS and Prawler to draw from (the engineering data wont initially be retrieved)\n",
    "- 7 deployments\n",
    "- Only daily data will be uploaded to JPL\n",
    "- there are time conversion issues that prevent the >last_n_days feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b6ee1-e76a-4bf3-a96f-0cde700d2d32",
   "metadata": {},
   "source": [
    "- load into pandas\n",
    "- adjust cond units\n",
    "- combine gps time\n",
    "- calculate float time\n",
    "- build xarray file (not CF compliant - follow reverse engr template)\n",
    "- input metadata / translate to metadata format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc268f29-8ecb-4067-aeab-1018cd826350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from erddapy import ERDDAP\n",
    "import xarray as xa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def get_data(server_url='http://redwing.pmel.noaa.gov:8080/erddap', SWOTUnitID=None, DataType=None, DateRange=['2020-01-01','2021-01-01']):\n",
    "    \"\"\"\n",
    "    \n",
    "    DataType: GPS, PRAWC, BARO\n",
    "    \n",
    "    DateRange: [yyyy-mm-ddTHH:MM:SSZ,yyyy-mm-ddTHH:MM:SSZ] or [yyyy-mm-dd,yyyy-mm-dd]\n",
    "    \"\"\"\n",
    "\n",
    "    e = ERDDAP(server=server_url,\n",
    "        protocol='tabledap',\n",
    "        response='csv')\n",
    "\n",
    "    constraints = {\n",
    "    'time>=': DateRange[0],\n",
    "    'time<=': DateRange[1],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(e.get_search_url(response='csv', search_for=f'{SWOTUnitID} {DataType}'))\n",
    "    except HTTPError:\n",
    "        print('No search results')\n",
    "        return\n",
    "\n",
    "    e.dataset_id=df['Dataset ID'][0]\n",
    "    e.constraints = constraints\n",
    "    \n",
    "    pdf = e.to_pandas(\n",
    "                index_col='time (UTC)',\n",
    "                parse_dates=True,\n",
    "                skiprows=(1,)  # units information can be dropped.\n",
    "            )\n",
    "\n",
    "    return pdf\n",
    "\n",
    "class make_base_netcdf():\n",
    "    \"\"\"\n",
    "    Designed for:\n",
    "        - Ingesting Prawler and GPS data for SWOT Misson\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, SWOTID='N001', praw_data=None, gps_data=None):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): Pandas DataFrame of mesurement data.\n",
    "            instrument_yaml (str, optional): yaml file with instrumentation meta attributes. Defaults to ''.\n",
    "            operation_yaml (str, optional): yaml file with cruise or mooring meta attributes. Defaults to ''.\n",
    "            operation_type (str, optional): Choose from 'mooring','ctd',''. Defaults to 'mooring'.\n",
    "            instrument_id (str, optional): [description]. Defaults to ''.\n",
    "            inst_shortname (str, optional): [description]. Defaults to ''.\n",
    "        \"\"\"\n",
    "        assert ~praw_data.empty, \"Must have a valid dataframe of prawler data\"\n",
    "        assert ~gps_data.empty, \"Must have a valid dataframe of gps data\"\n",
    "        self.praw_data = praw_data\n",
    "        self.gps_data = gps_data\n",
    "        \n",
    "    def to_xarray(self):\n",
    "        self.praw_data = self.praw_data.rename(\n",
    "            columns={\n",
    "                        'SB_Temp':'TEMP',\n",
    "                        'SB_Depth':'PRES',\n",
    "                        'SB_Conductivity':'CNDC',}\n",
    "        )\n",
    "        self.praw_data.index.name = 'TIME'\n",
    "\n",
    "        self.praw_data['TIME'] = self.praw_data.index.values\n",
    "        #drop all others\n",
    "        afg = self.praw_data.to_xarray()\n",
    "        afg = afg.drop_indexes('TIME')\n",
    "        for x in afg.var():\n",
    "            if x not in ['PRES','TEMP','CNDC']:\n",
    "                afg = afg.drop(x)\n",
    "        afg = afg.drop('Epoch_Time')        \n",
    "        \n",
    "        self.praw_data = afg\n",
    "\n",
    "        return(self.praw_data,self.gps_data)\n",
    "        \n",
    "    def add_deploy_location(self, dep_lat_n = 34.156113, dep_lon_e = -118.131943):\n",
    "        self.praw_data = self.praw_data.assign_coords({'LATITUDE': SWOTSerialID[SWOTUnit]['DepLatN'], 'LONGITUDE': SWOTSerialID[SWOTUnit]['DepLonE']}) \\\n",
    "         .expand_dims({'LATITUDE': 1, 'LONGITUDE':1})\n",
    "        self.praw_data['PRES']=self.praw_data.PRES.isel({'LATITUDE':0,'LONGITUDE':0})\n",
    "        self.praw_data['TEMP']=self.praw_data.TEMP.isel({'LATITUDE':0,'LONGITUDE':0})\n",
    "        self.praw_data['CNDC']=self.praw_data.CNDC.isel({'LATITUDE':0,'LONGITUDE':0})\n",
    "        \n",
    "    def add_gps_data(self):\n",
    "        self.gps_data['LatD'] = self.gps_data['LatD'].apply(lambda x: 1 if x == 'N' else -1)\n",
    "        self.gps_data['LonD'] = self.gps_data['LonD'].apply(lambda x: 1 if x == 'E' else -1)\n",
    "        self.gps_data['LatDD'] = self.gps_data['LatD']*((self.gps_data['Latitude']/100-np.floor(self.gps_data['Latitude']/100))*100 / 60) + self.gps_data['Latitude']/100\n",
    "        self.gps_data['LonDD'] = self.gps_data['LonD']*(((self.gps_data['Longitude']/100-np.floor(self.gps_data['Longitude']/100))*100 / 60) + self.gps_data['Longitude']/100)\n",
    "    \n",
    "        return self.gps_data\n",
    "    \n",
    "    def add_variable_meta(self):\n",
    "        pass\n",
    "    \n",
    "    def inst_meta(self):\n",
    "        pass\n",
    "    \n",
    "    def global_attr(self):\n",
    "        pass\n",
    "    \n",
    "    def history(self):\n",
    "        pass\n",
    "    \n",
    "    def xarray2netcdf_save(self, xdf, filename='temp.nc', **kwargs):\n",
    "        \"\"\"Save xarray to netcdf\n",
    "\n",
    "        Args:\n",
    "            xdf (xarray dataset): xarray dataset\n",
    "            filename (str, optional): Filename. Defaults to 'temp.nc'.\n",
    "        \"\"\"\n",
    "                \n",
    "        self.praw_data.to_netcdf(filename,format=kwargs['format'],encoding={'TIME':{'units':'days since 2000-01-01'}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c60257b-7be8-4005-bd6d-6020292cae48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N001\n"
     ]
    }
   ],
   "source": [
    "# datarange = \n",
    "\n",
    "for SWOTUnit in SWOTSerialID:\n",
    "    print(SWOTUnit)\n",
    "    dfg = {}\n",
    "    datastatus = 0 #0: no data processed, 1: prawler/gps dataprocessed, 2: prawler and gps data processed\n",
    "    \n",
    "    for datatype in ['PRAWC','GPS']:\n",
    "        pdf = get_data(SWOTUnitID=SWOTUnit, DataType=datatype, DateRange=['2020-1-01','2020-02-01'])\n",
    "        dfg.update({SWOTUnit+datatype:pdf})\n",
    "        \n",
    "\n",
    "        dfg[SWOTUnit+datatype].index.name = dfg[SWOTUnit+datatype].index.name.split(' ')[0]\n",
    "        dfg[SWOTUnit+datatype].columns = [x.split(' ')[0] for x in dfg[SWOTUnit+datatype].columns]\n",
    "        \n",
    "        # dfg[SWOTUnit+datatype] = dfg[SWOTUnit+datatype].loc['2020':'2020'] #<--- hard coded sanity check for SWOT data in period of comparision\n",
    "        \n",
    "        if datatype == 'PRAWC':\n",
    "            dfg[SWOTUnit+datatype] = dfg[SWOTUnit+datatype].rename(\n",
    "                columns={\n",
    "                            'SB_Temp':'TEMP',\n",
    "                            'SB_Depth':'PRES',\n",
    "                            'SB_Conductivity':'CNDC',}\n",
    "            )\n",
    "            dfg[SWOTUnit+datatype].index.name = 'TIME'\n",
    "            \n",
    "            dfg[SWOTUnit+datatype]['TIME'] = dfg[SWOTUnit+datatype].index.values\n",
    "            dfg[SWOTUnit+datatype].sort_index(inplace=True)\n",
    "            #drop all others\n",
    "            afg = dfg[SWOTUnit+datatype].to_xarray()\n",
    "            afg = afg.drop_indexes('TIME')\n",
    "            \n",
    "            for x in afg.var():\n",
    "                if x not in ['PRES','TEMP','CNDC']:\n",
    "                    afg = afg.drop(x)\n",
    "            afg = afg.drop('Epoch_Time')\n",
    "        \n",
    "            #add Latitude/Longitude from Deployment\n",
    "            afg = afg.assign_coords({'LATITUDE': SWOTSerialID[SWOTUnit]['DepLatN'], 'LONGITUDE': SWOTSerialID[SWOTUnit]['DepLonE']}) \\\n",
    "                     .expand_dims({'LATITUDE': 1, 'LONGITUDE':1})\n",
    "            afg['PRES']=afg.PRES.isel({'LATITUDE':0,'LONGITUDE':0})\n",
    "            afg['TEMP']=afg.TEMP.isel({'LATITUDE':0,'LONGITUDE':0})\n",
    "            afg['CNDC']=afg.CNDC.isel({'LATITUDE':0,'LONGITUDE':0})\n",
    "\n",
    "            #add ancillary info\n",
    "            afg = afg.assign_coords({'INSTR_INFO':np.array('',dtype='S1')})\n",
    "            afg = afg.assign_coords({'INSTR_MAKE': np.array(SWOTSerialID[SWOTUnit]['InstMake'],dtype='S19')})\n",
    "            afg = afg.assign_coords({'INSTR_MODEL': np.array(SWOTSerialID[SWOTUnit]['InstModel'],dtype='S5')})\n",
    "            afg = afg.assign_coords({'INSTR_SN': np.array(SWOTSerialID[SWOTUnit]['InstSerial'],dtype='int')})\n",
    "\n",
    "            # odd cleanup for formatting\n",
    "            afg = afg.drop_duplicates(dim='TIME')\n",
    "            afg['CNDC'] = afg.CNDC  * 10 #CC unit conversion\n",
    "        \n",
    "            datastatus += 1\n",
    "        #add gps info\n",
    "        if datatype == 'GPS':\n",
    "            core = make_base_netcdf(praw_data=dfg[SWOTUnit+'PRAWC'], gps_data=dfg[SWOTUnit+'GPS'])\n",
    "\n",
    "            t,g = core.to_xarray()\n",
    "            gps = core.add_gps_data()\n",
    "\n",
    "            gps_df = g[['LatDD','LonDD']]\n",
    "\n",
    "            gps_df.index.name = 'TIME_SURFACE_BUOY'\n",
    "            gps_df = gps_df.rename(\n",
    "                            columns={\n",
    "                                        'LatDD':'LATITUDE_SURFACE_BUOY',\n",
    "                                        'LonDD':'LONGITUDE_SURFACE_BUOY',}\n",
    "                            )\n",
    "\n",
    "            gps_df['TIME_SURFACE_BUOY'] = gps_df.index.values\n",
    "            gps_df.sort_index(inplace=True)\n",
    "\n",
    "            gps_xdf = gps_df.to_xarray()\n",
    "            # odd cleanup for formatting\n",
    "            gps_xdf = gps_xdf.drop_duplicates(dim='TIME_SURFACE_BUOY')\n",
    "\n",
    "            datastatus += 1\n",
    "\n",
    "        if datastatus == 2:    \n",
    "            afg = afg.merge(gps_xdf)\n",
    "\n",
    "            #add var meta information\n",
    "            for var in afg.var():\n",
    "                afg[var].attrs = SWOTprawler[var]\n",
    "                afg[var].encoding['_FillValue'] = None\n",
    "            for var in afg.coords:\n",
    "                afg[var].attrs = SWOTprawler[var]\n",
    "                afg[var].encoding['_FillValue'] = None\n",
    "\n",
    "            #add global meta information\n",
    "            afg.attrs.update(SWOTGlobal['erddap_globalattributes'])\n",
    "\n",
    "            #add dynamic meta information\n",
    "            afg.attrs.update({'date_created':datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")})\n",
    "            afg.attrs.update({'geospatial_lat_min':afg.LATITUDE_SURFACE_BUOY.min().values})\n",
    "            afg.attrs.update({'geospatial_lat_max':afg.LATITUDE_SURFACE_BUOY.max().values})\n",
    "            afg.attrs.update({'geospatial_lon_min':afg.LONGITUDE_SURFACE_BUOY.min().values})\n",
    "            afg.attrs.update({'geospatial_lon_max':afg.LONGITUDE_SURFACE_BUOY.max().values})\n",
    "            afg.attrs.update({'geospatial_vertical_min':afg.PRES.min().values})\n",
    "            afg.attrs.update({'geospatial_vertical_max':afg.PRES.max().values})\n",
    "            afg.attrs.update({'time_coverage_start':afg.TIME.min().values.astype('datetime64[s]').item().strftime(\"%Y%m%dZ%H%M%SZ\")})\n",
    "            afg.attrs.update({'time_coverage_end':afg.TIME.max().values.astype('datetime64[s]').item().strftime(\"%Y%m%dZ%H%M%SZ\")})\n",
    "\n",
    "            afg.attrs.update({'history':afg.attrs['history'] + datetime.datetime.utcnow().strftime(\"%Y-%m-%d\") +': Initial File Creation - RUDICS/FLOT to NETCDF Conversion TEST for SWOT Validation'}) \n",
    "            \n",
    "            afg = afg.reset_coords(names=[\"INSTR_INFO\",\"INSTR_MAKE\",\"INSTR_MODEL\",\"INSTR_SN\"])\n",
    "            afg.to_netcdf('PMEL_SWOTtest_dataset.nc',encoding={'TIME':{'units':'days since 20000101T000000Z','_FillValue':None},\n",
    "                                                               'TIME_SURFACE_BUOY':{'units':'days since 20000101T000000Z','_FillValue':None}})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f50332-d68d-4e09-8a67-8da443017ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-py310]",
   "language": "python",
   "name": "conda-env-miniconda3-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
