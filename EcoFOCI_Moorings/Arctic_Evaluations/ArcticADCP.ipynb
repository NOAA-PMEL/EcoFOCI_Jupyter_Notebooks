{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations of Arctic ADCP Data 2016-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example step by step for one site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://akutan.pmel.noaa.gov:8080/erddap/griddap/ADCP_Mooring_18bsp8a_final\n"
     ]
    }
   ],
   "source": [
    "from erddapy import ERDDAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "server_url='http://akutan.pmel.noaa.gov:8080/erddap'\n",
    "dataset = 'ADCP_Mooring_18bsp8a_final'\n",
    "\n",
    "e = ERDDAP(server=server_url)\n",
    "\n",
    "e.constraints = None\n",
    "e.protocol = 'griddap'\n",
    "\n",
    "opendap_url = e.get_download_url(\n",
    "    dataset_id=dataset,\n",
    "    response='opendap',\n",
    ")\n",
    "\n",
    "print(opendap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "curl error details: \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -68] NetCDF: I/O failure: b'http://akutan.pmel.noaa.gov:8080/erddap/griddap/ADCP_Mooring_18bsp8a_final'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vv/blq095kj0xj9nz1v0ffdwfch0000gp/T/ipykernel_22125/295652358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Import succeeded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopendap_url\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -68] NetCDF: I/O failure: b'http://akutan.pmel.noaa.gov:8080/erddap/griddap/ADCP_Mooring_18bsp8a_final'"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "### final only with filtered data\n",
    "import sys\n",
    "try:\n",
    "    # The insertion index should be 1 because index 0 is this file\n",
    "    sys.path.insert(1, '/Users/bell/Programs/Python/EcoFOCI_Jupyter_Notebooks/filters')  # the type of path is string\n",
    "    # because the system path already have the absolute path to folder a\n",
    "    # so it can recognize file_a.py while searching \n",
    "    import lanzcos\n",
    "except (ModuleNotFoundError, ImportError) as e:\n",
    "    print(\"{} fileure\".format(type(e)))\n",
    "else:\n",
    "    print(\"Import succeeded\")\n",
    "    \n",
    "with Dataset(opendap_url) as nc:\n",
    "    print(nc.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.dataset_id=dataset\n",
    "e.response = 'nc'\n",
    "ds = e.to_xarray(decode_times=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get second to last index (so 2 bin from bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_curr = ds.isel(depth=ds.depth.size-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "udata = depth_curr.u_1205[:,0,0].to_pandas()\n",
    "vdata = depth_curr.v_1206[:,0,0].to_pandas()\n",
    "\n",
    "data = pd.concat([udata, vdata], axis=1)\n",
    "data.columns = ['udata','vdata']\n",
    "\n",
    "data['udata'][data.udata > 1e34]=np.nan\n",
    "data['vdata'][data.vdata > 1e34]=np.nan\n",
    "data = data.resample('1H').mean()\n",
    "\n",
    "## missing data fill limited by number of hours to linearly interpolate\n",
    "data['tempu'] = data['udata'].interpolate(method='time',limit=6)\n",
    "data['tempv'] = data['vdata'].interpolate(method='time',limit=6)\n",
    "\n",
    "# get index of missing data that wasn't filled\n",
    "# the remaining missing data will continue to be missing after analysis\n",
    "# this data and the edges of the timeseries need to have the effects of the filter window accounted for\n",
    "\n",
    "missing_index_u = np.isnan(data.tempu)\n",
    "missing_index_v = np.isnan(data.tempv)\n",
    "data['tempu'] = data['udata'].interpolate(method='time')\n",
    "data['tempv'] = data['vdata'].interpolate(method='time')\n",
    "\n",
    "#filter data with rudimentary matlab script\n",
    "data['ufdata'] = lanzcos.lanzcos(data.tempu.values,1,35)+data['udata'].mean()\n",
    "data['vfdata'] = lanzcos.lanzcos(data.tempv.values,1,35)+data['vdata'].mean()\n",
    "data['ufdata'][missing_index_u] = np.nan\n",
    "data['vfdata'][missing_index_v] = np.nan    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cmocean\n",
    "print(cmocean.cm.cmapnames)\n",
    "\n",
    "from matplotlib.dates import (\n",
    "    YearLocator,\n",
    "    WeekdayLocator,\n",
    "    MonthLocator,\n",
    "    DayLocator,\n",
    "    HourLocator,\n",
    "    DateFormatter,\n",
    ")\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 2))\n",
    "data.udata.plot(ax=ax)\n",
    "data.ufdata.plot(ax=ax,color='r')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 2))\n",
    "data.vdata.plot(ax=ax)\n",
    "data.vfdata.plot(ax=ax,color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timeseries1dStickPlot(object):\n",
    "\n",
    "    mpl.rcParams['svg.fonttype'] = 'none'\n",
    "    mpl.rcParams['ps.fonttype'] = 42\n",
    "    mpl.rcParams['pdf.fonttype'] = 42\n",
    "    \n",
    "    def __init__(self, fontsize=10, labelsize=10, plotstyle='k-.', stylesheet='bmh'):\n",
    "        \"\"\"Initialize the timeseries with items that do not change.\n",
    "\n",
    "        This sets up the axes and station locations. The `fontsize` and `spacing`\n",
    "        are also specified here to ensure that they are consistent between individual\n",
    "        station elements.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fontsize : int\n",
    "            The fontsize to use for drawing text\n",
    "        labelsize : int\n",
    "          The fontsize to use for labels\n",
    "        stylesheet : str\n",
    "          Choose a mpl stylesheet [u'seaborn-darkgrid', \n",
    "          u'seaborn-notebook', u'classic', u'seaborn-ticks', \n",
    "          u'grayscale', u'bmh', u'seaborn-talk', u'dark_background', \n",
    "          u'ggplot', u'fivethirtyeight', u'seaborn-colorblind', \n",
    "          u'seaborn-deep', u'seaborn-whitegrid', u'seaborn-bright', \n",
    "          u'seaborn-poster', u'seaborn-muted', u'seaborn-paper', \n",
    "          u'seaborn-white', u'seaborn-pastel', u'seaborn-dark', \n",
    "          u'seaborn-dark-palette']\n",
    "        \"\"\"\n",
    "\n",
    "        self.fontsize = fontsize\n",
    "        self.labelsize = labelsize\n",
    "        self.plotstyle = plotstyle\n",
    "        plt.style.use(stylesheet)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_title(mooringid='',lat=-99.9,lon=-99.9,depth=9999,instrument=''):\n",
    "      \"\"\"Pass parameters to annotate the title of the plot\n",
    "\n",
    "      This sets the standard plot title using common meta information from PMEL/EPIC style netcdf files\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      mooringid : str\n",
    "        Mooring Identifier\n",
    "      lat : float\n",
    "        The latitude of the mooring\n",
    "      lon : float\n",
    "        The longitude of the mooring\n",
    "      depth : int\n",
    "        Nominal depth of the instrument\n",
    "      instrument : str\n",
    "        Name/identifier of the instrument plotted\n",
    "      \"\"\"  \n",
    "      ptitle = (\"Plotted on: {time:%Y/%m/%d %H:%M} \\n from {mooringid} Lat: {latitude:3.3f}  Lon: {longitude:3.3f}\" \n",
    "            \" Depth: {depth}\\n : {instrument}\").format(\n",
    "            time=datetime.datetime.now(), \n",
    "                  mooringid=mooringid,\n",
    "                  latitude=lat, \n",
    "                  longitude=lon, \n",
    "                  depth=depth,\n",
    "                  instrument=instrument )\n",
    "\n",
    "      return ptitle\n",
    "\n",
    "\n",
    "    def plot(self, timedata=None, udata=None, vdata=None, ylabel=None, linescale=1, **kwargs):\n",
    "\n",
    "      if kwargs['rotate'] != 0.0:\n",
    "          #when rotating vectors - positive(+) rotation is equal to cw of the axis (ccw of vector)\n",
    "          #                      - negative(+) rotation is equal to ccw of the axis (cw of the vector)\n",
    "          print(\"rotating vectors\")\n",
    "          angle_offset_rad = np.deg2rad(kwargs['rotate'])\n",
    "          udata = udata*np.cos(angle_offset_rad) + vdata*np.sin(angle_offset_rad)\n",
    "          vdata = -1.*udata*np.sin(angle_offset_rad) + vdata*np.cos(angle_offset_rad)\n",
    "\n",
    "      magnitude = np.sqrt(udata**2 + vdata**2)\n",
    "\n",
    "      fig, (ax1,ax2) = plt.subplots(2,1,sharex='col',figsize=(11,4.25))\n",
    "\n",
    "\n",
    "      # Plot u and v components\n",
    "      # Plot quiver\n",
    "      ax1.set_ylim(-1*np.nanmax(magnitude), np.nanmax(magnitude))\n",
    "      fill1 = ax1.fill_between(timedata, magnitude, 0, color='k', alpha=0.1)\n",
    "\n",
    "      # Fake 'box' to be able to insert a legend for 'Magnitude'\n",
    "      \"\"\"\n",
    "      p = ax1.add_patch(plt.Rectangle((1,1),1,1,fc='k',alpha=0.1))\n",
    "      leg1 = ax1.legend([p], [\"Current magnitude [cm/s]\"],loc='lower right')\n",
    "      leg1._drawFrame=False\n",
    "      \"\"\"\n",
    "\n",
    "      # 1D Quiver plot\n",
    "      q = ax1.quiver(timedata,0,udata,vdata,color='r',units='y',scale_units='y',\n",
    "                     scale = 1,headlength=1,headaxislength=1,width=0.04*linescale,alpha=.95)\n",
    "      qk = plt.quiverkey(q,0.2, 0.05, 5,r'$5 \\frac{cm}{s}$',labelpos='W',\n",
    "                     fontproperties={'weight': 'bold'})\n",
    "\n",
    "\n",
    "      # Plot u and v components\n",
    "      ax1.set_xticklabels(ax1.get_xticklabels(), visible=False)\n",
    "      ax2.set_xticklabels(ax2.get_xticklabels(), visible=True)\n",
    "      ax1.axes.get_xaxis().set_visible(False)\n",
    "      ax1.set_xlim(timedata.min(),timedata.max())\n",
    "      ax1.set_ylabel(\"Velocity (cm/s)\")\n",
    "      ax2.plot(timedata, vdata, 'b-', linewidth=0.25)\n",
    "      ax2.plot(timedata, udata, 'g-', linewidth=0.25)\n",
    "      ax2.set_xlim(timedata.min(),timedata.max())\n",
    "      ax2.set_xlabel(\"Date (UTC)\")\n",
    "      ax2.set_ylabel(\"Velocity (cm/s)\")\n",
    "      ax2.xaxis.set_major_locator(MonthLocator())\n",
    "      ax2.xaxis.set_minor_locator(MonthLocator(bymonth=range(1,13), bymonthday=15))\n",
    "      ax2.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "      ax2.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "      ax1.spines['bottom'].set_visible(False)\n",
    "      ax2.spines['top'].set_visible(False)\n",
    "      ax1.xaxis.set_ticks_position('top')\n",
    "      ax2.xaxis.set_ticks_position('bottom')\n",
    "      ax2.yaxis.set_ticks_position('both')\n",
    "      ax2.tick_params(axis='both', which='minor', labelsize=self.labelsize)\n",
    "      ax1.tick_params(axis='both', which='minor', labelsize=self.labelsize)\n",
    "      #manual time limit sets\n",
    "      #ax1.set_xlim([datetime.datetime(2016,2,1),datetime.datetime(2016,9,15)])\n",
    "      #ax2.set_xlim([datetime.datetime(2016,2,1),datetime.datetime(2016,9,15)])\n",
    "      # Set legend location - See: http://matplotlib.org/Volumes/WDC_internal/users/legend_guide.html#legend-location\n",
    "      leg2 = plt.legend(['v','u'],loc='upper left')\n",
    "      leg2._drawFrame=False\n",
    "\n",
    "\n",
    "      return plt, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Timeseries1dStickPlot()\n",
    "plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                     udata=data.ufdata.values, \n",
    "                     vdata=data.vfdata.values,\n",
    "                     rotate=0)\n",
    "plt1.xlabel('test')\n",
    "fig1.savefig('test_f35'+'.png',dpi=600)\n",
    "\n",
    "\n",
    "#subsample\n",
    "datasub=data.resample('24H').median()\n",
    "p1 = Timeseries1dStickPlot()\n",
    "plt1, fig1 = p1.plot(timedata=datasub.index, \n",
    "                     udata=datasub.ufdata.values, \n",
    "                     vdata=datasub.vfdata.values,\n",
    "                     linescale=10,\n",
    "                     rotate=0)\n",
    "plt1.xlabel('test')\n",
    "fig1.savefig('test_f35_12hr'+'.png',dpi=600)\n",
    "\n",
    "p1 = Timeseries1dStickPlot()\n",
    "plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                     udata=data.udata.values, \n",
    "                     vdata=data.vdata.values,\n",
    "                     rotate=0)\n",
    "plt1.xlabel('test')\n",
    "fig1.savefig('test_nofilter'+'.png',dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through ADCP all sites\n",
    "\n",
    "### Determine Mean, STD, Max value for each deployment\n",
    "\n",
    "- from filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_url='http://akutan.pmel.noaa.gov:8080/erddap'\n",
    "\n",
    "e = ERDDAP(server=server_url)\n",
    "\n",
    "e.protocol = 'griddap'\n",
    "\n",
    "opendap_url = e.get_download_url(\n",
    "    dataset_id=dataset,\n",
    "    response='opendap',\n",
    ")\n",
    "\n",
    "print(opendap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one depth\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plotfigs = False\n",
    "savedata = True\n",
    "domath = False\n",
    "\n",
    "for dataset_id in sorted([dataset]):\n",
    "    try:\n",
    "        e = ERDDAP(server=server_url,\n",
    "            protocol='griddap',\n",
    "            response='opendap'\n",
    "        )\n",
    "        e.constraints=None\n",
    "        e.protocol = 'griddap'\n",
    "\n",
    "        e.dataset_id=dataset_id\n",
    "        e.response = 'nc'\n",
    "        ds = e.to_xarray(decode_times=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    depth_curr = ds.isel(depth=ds.depth.size-2)\n",
    "    \n",
    "    ### filter\n",
    "    udata = depth_curr.u_1205[:,0,0].to_pandas()\n",
    "    vdata = depth_curr.v_1206[:,0,0].to_pandas()\n",
    "\n",
    "    data = pd.concat([udata, vdata], axis=1)\n",
    "    data.columns = ['udata','vdata']\n",
    "\n",
    "    data['udata'][data.udata > 1e34]=np.nan\n",
    "    data['vdata'][data.vdata > 1e34]=np.nan\n",
    "    data = data.resample('1H').mean()\n",
    "\n",
    "    ## missing data fill limited by number of hours to linearly interpolate\n",
    "    data['tempu'] = data['udata'].interpolate(method='time',limit=6)\n",
    "    data['tempv'] = data['vdata'].interpolate(method='time',limit=6)\n",
    "\n",
    "    # get index of missing data that wasn't filled\n",
    "    # the remaining missing data will continue to be missing after analysis\n",
    "    # this data and the edges of the timeseries need to have the effects of the filter window accounted for\n",
    "\n",
    "    missing_index_u = np.isnan(data.tempu)\n",
    "    missing_index_v = np.isnan(data.tempv)\n",
    "    data['tempu'] = data['udata'].interpolate(method='time')\n",
    "    data['tempv'] = data['vdata'].interpolate(method='time')\n",
    "\n",
    "    #filter data with rudimentary matlab script\n",
    "    data['ufdata'] = lanzcos.lanzcos(data.tempu.values,1,35)+data['udata'].mean()\n",
    "    data['vfdata'] = lanzcos.lanzcos(data.tempv.values,1,35)+data['vdata'].mean()\n",
    "    data['ufdata'][missing_index_u] = np.nan\n",
    "    data['vfdata'][missing_index_v] = np.nan    \n",
    "\n",
    "    #extra calculations\n",
    "    if domath:\n",
    "        print(dataset_id)\n",
    "        print(data.describe()[['ufdata','vfdata']])\n",
    "        \n",
    "    #plot\n",
    "    if plotfigs:\n",
    "        p1 = Timeseries1dStickPlot()\n",
    "        plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                             udata=data.ufdata.values, \n",
    "                             vdata=data.vfdata.values,\n",
    "                             rotate=0)\n",
    "        plt1.xlabel(dataset_id+'_f35')\n",
    "        fig1.savefig(dataset_id+'_f35'+'.png',dpi=600)\n",
    "\n",
    "\n",
    "        #subsample\n",
    "        datasub=data.resample('D').median()\n",
    "        p1 = Timeseries1dStickPlot()\n",
    "        plt1, fig1 = p1.plot(timedata=datasub.index, \n",
    "                             udata=datasub.ufdata.values, \n",
    "                             vdata=datasub.vfdata.values,\n",
    "                             linescale=10,\n",
    "                             rotate=0)\n",
    "        plt1.xlabel(dataset_id+'_f35_daily')\n",
    "        fig1.savefig(dataset_id+'_f35_daily'+'.png',dpi=600)\n",
    "\n",
    "        p1 = Timeseries1dStickPlot()\n",
    "        plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                             udata=data.udata.values, \n",
    "                             vdata=data.vdata.values,\n",
    "                             rotate=0)\n",
    "        plt1.xlabel(dataset_id+'_nofilter')\n",
    "        fig1.savefig(dataset_id+'_nofilter'+'.png',dpi=600)\n",
    "        \n",
    "    #save data\n",
    "    if savedata:\n",
    "        data[['ufdata','vfdata']].to_csv(dataset_id+'_f35'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all depths but to individual files\n",
    "\n",
    "from requests.exceptions import HTTPError\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plotfigs = True\n",
    "savedata = True\n",
    "domath = False\n",
    "\n",
    "for dataset_id in sorted([dataset]):\n",
    "    try:\n",
    "        e = ERDDAP(server=server_url,\n",
    "            protocol='griddap',\n",
    "            response='opendap'\n",
    "        )\n",
    "        e.constraints=None\n",
    "        e.protocol = 'griddap'\n",
    "\n",
    "        e.dataset_id=dataset_id\n",
    "        e.response = 'nc'\n",
    "        ds = e.to_xarray(decode_times=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for dd in range(ds.depth.size-1,-1,-1):\n",
    "        depth_curr = ds.isel(depth=dd).sel(time=slice('2018-10-12','2020'))\n",
    "\n",
    "        ### filter\n",
    "        udata = depth_curr.u_1205[:,0,0].to_pandas()\n",
    "        vdata = depth_curr.v_1206[:,0,0].to_pandas()\n",
    "\n",
    "        data = pd.concat([udata, vdata], axis=1)\n",
    "        data.columns = ['udata','vdata']\n",
    "\n",
    "        data['udata'][data.udata > 1e34]=np.nan\n",
    "        data['vdata'][data.vdata > 1e34]=np.nan\n",
    "        data = data.resample('1H').mean()\n",
    "\n",
    "        ## missing data fill limited by number of hours to linearly interpolate\n",
    "        data['tempu'] = data['udata'].interpolate(method='time',limit=6)\n",
    "        data['tempv'] = data['vdata'].interpolate(method='time',limit=6)\n",
    "\n",
    "        # get index of missing data that wasn't filled\n",
    "        # the remaining missing data will continue to be missing after analysis\n",
    "        # this data and the edges of the timeseries need to have the effects of the filter window accounted for\n",
    "\n",
    "        missing_index_u = np.isnan(data.tempu)\n",
    "        missing_index_v = np.isnan(data.tempv)\n",
    "        data['tempu'] = data['udata'].interpolate(method='time')\n",
    "        data['tempv'] = data['vdata'].interpolate(method='time')\n",
    "\n",
    "        #filter data with rudimentary matlab script\n",
    "        data['ufdata'] = lanzcos.lanzcos(data.tempu.values,1,35)+data['udata'].mean()\n",
    "        data['vfdata'] = lanzcos.lanzcos(data.tempv.values,1,35)+data['vdata'].mean()\n",
    "        data['ufdata'][missing_index_u] = np.nan\n",
    "        data['vfdata'][missing_index_v] = np.nan    \n",
    "\n",
    "        #extra calculations\n",
    "        if domath:\n",
    "            print(dataset_id)\n",
    "            print(data.describe()[['ufdata','vfdata']])\n",
    "\n",
    "        #plot\n",
    "        if plotfigs:\n",
    "            try:\n",
    "                p1 = Timeseries1dStickPlot()\n",
    "                plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                                     udata=data.ufdata.values, \n",
    "                                     vdata=data.vfdata.values,\n",
    "                                     rotate=0)\n",
    "                plt1.xlabel(dataset_id+'_f35')\n",
    "                fig1.savefig(dataset_id+'_f35'+'.png',dpi=600)\n",
    "\n",
    "\n",
    "                #subsample\n",
    "                datasub=data.resample('D').median()\n",
    "                p1 = Timeseries1dStickPlot()\n",
    "                plt1, fig1 = p1.plot(timedata=datasub.index, \n",
    "                                     udata=datasub.ufdata.values, \n",
    "                                     vdata=datasub.vfdata.values,\n",
    "                                     linescale=10,\n",
    "                                     rotate=0)\n",
    "                plt1.xlabel(dataset_id+'_f35_daily')\n",
    "                fig1.savefig(dataset_id+'_f35_daily'+'.png',dpi=600)\n",
    "\n",
    "                p1 = Timeseries1dStickPlot()\n",
    "                plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                                     udata=data.udata.values, \n",
    "                                     vdata=data.vdata.values,\n",
    "                                     rotate=0)\n",
    "                plt1.xlabel(dataset_id+'_nofilter')\n",
    "                fig1.savefig(dataset_id+'_nofilter'+'.png',dpi=600)\n",
    "            except:\n",
    "                pass\n",
    "        #save data\n",
    "        if savedata:\n",
    "            data[['ufdata','vfdata']].to_csv(dataset_id+'_'+str(int(ds.depth[dd].values)).zfill(2)+'_f35'+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2012/2016 C1** had no ADCP data so do analysis on Seaguard data\n",
    "\n",
    "**2011 C1 was QC'd to one depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = '16ckip1a_sg_0039'\n",
    "depth_curr = xr.load_dataset('/Users/bell/ecoraid/2016/Moorings/16ckip1a/erddap/initial_archive/16ckip1a_sg_0039.unqcd.cf.nc')\n",
    "dataset_id = '11ckp1a_wcp_vel_0028m'\n",
    "depth_curr = xr.load_dataset('/Users/bell/ecoraid/2011/Moorings/11ckp1a/erddap/final_data/11ckp1a_wcp_vel_0028m.cf.nc')\n",
    "dataset_id = '12ckp2a_wcp_vel_0032m'\n",
    "depth_curr = xr.load_dataset('/Users/bell/ecoraid/2012/Moorings/12ckp2a/erddap/final_data/12ckp2a_wcp_vel_0032m.cf.nc')\n",
    "dataset_id = '12ckip2a_an9_0041m'\n",
    "depth_curr = xr.load_dataset('/Users/bell/ecoraid/2012/Moorings/12ckip2a/erddap/final_data/12ckip2a_an9_0041m.cf.nc')\n",
    "\n",
    "depth_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.u_1205.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plotfigs = True\n",
    "savedata = True\n",
    "domath = True\n",
    "\n",
    "\n",
    "### dropna's to start (gets rid of any where all entries are gone)\n",
    "depth_curr = depth_curr.dropna(dim='time',how=\"all\")\n",
    "\n",
    "### filter\n",
    "udata = depth_curr.U_320[:,0,0].to_pandas()\n",
    "vdata = depth_curr.V_321[:,0,0].to_pandas()\n",
    "\n",
    "data = pd.concat([udata, vdata], axis=1)\n",
    "data.columns = ['udata','vdata']\n",
    "\n",
    "data['udata'][data.udata > 1e34]=np.nan\n",
    "data['vdata'][data.vdata > 1e34]=np.nan\n",
    "data = data.resample('1H').mean()\n",
    "\n",
    "## missing data fill limited by number of hours to linearly interpolate\n",
    "data['tempu'] = data['udata'].interpolate(method='time',limit=6)\n",
    "data['tempv'] = data['vdata'].interpolate(method='time',limit=6)\n",
    "\n",
    "# get index of missing data that wasn't filled\n",
    "# the remaining missing data will continue to be missing after analysis\n",
    "# this data and the edges of the timeseries need to have the effects of the filter window accounted for\n",
    "\n",
    "missing_index_u = np.isnan(data.tempu)\n",
    "missing_index_v = np.isnan(data.tempv)\n",
    "data['tempu'] = data['udata'].interpolate(method='time')\n",
    "data['tempv'] = data['vdata'].interpolate(method='time')\n",
    "\n",
    "#filter data with rudimentary matlab script\n",
    "data['ufdata'] = lanzcos.lanzcos(data.tempu.values,1,35)+data['udata'].mean()\n",
    "data['vfdata'] = lanzcos.lanzcos(data.tempv.values,1,35)+data['vdata'].mean()\n",
    "data['ufdata'][missing_index_u] = np.nan\n",
    "data['vfdata'][missing_index_v] = np.nan    \n",
    "\n",
    "#extra calculations\n",
    "if domath:\n",
    "    print(dataset_id)\n",
    "    print(data.describe()[['ufdata','vfdata']])\n",
    "\n",
    "#plot\n",
    "if plotfigs:\n",
    "    p1 = Timeseries1dStickPlot()\n",
    "    plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                         udata=data.ufdata.values, \n",
    "                         vdata=data.vfdata.values,\n",
    "                         rotate=0)\n",
    "    plt1.xlabel(dataset_id+'_f35')\n",
    "    fig1.savefig(dataset_id+'_f35'+'.png',dpi=600)\n",
    "\n",
    "\n",
    "    #subsample\n",
    "    datasub=data.resample('D').median()\n",
    "    p1 = Timeseries1dStickPlot()\n",
    "    plt1, fig1 = p1.plot(timedata=datasub.index, \n",
    "                         udata=datasub.ufdata.values, \n",
    "                         vdata=datasub.vfdata.values,\n",
    "                         linescale=10,\n",
    "                         rotate=0)\n",
    "    plt1.xlabel(dataset_id+'_f35_daily')\n",
    "    fig1.savefig(dataset_id+'_f35_daily'+'.png',dpi=600)\n",
    "\n",
    "    p1 = Timeseries1dStickPlot()\n",
    "    plt1, fig1 = p1.plot(timedata=data.index, \n",
    "                         udata=data.udata.values, \n",
    "                         vdata=data.vdata.values,\n",
    "                         rotate=0)\n",
    "    plt1.xlabel(dataset_id+'_nofilter')\n",
    "    fig1.savefig(dataset_id+'_nofilter'+'.png',dpi=600)\n",
    "\n",
    "#save data\n",
    "if savedata:\n",
    "    data[['ufdata','vfdata']].to_csv(dataset_id+'_f35'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
